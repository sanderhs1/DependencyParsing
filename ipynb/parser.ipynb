{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29e92963-56f9-4d8c-b780-28c371625263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel, \n",
    "    AutoModelForMaskedLM, \n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoModelForTokenClassification\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e912ae7-c199-4869-b151-3e0f9deaaaae",
   "metadata": {},
   "source": [
    "# Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "682b606d-39db-47de-9160-7fb4f9f1cf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_1d(tensor: torch.Tensor, length: int, pad_value: int = 0):\n",
    "    if tensor.size(0) >= length:\n",
    "        return tensor[:length]\n",
    "    else:\n",
    "        return torch.cat([tensor, torch.full((length - tensor.size(0),), pad_value, dtype=tensor.dtype)], dim=0)\n",
    "\n",
    "def pad_2d(tensor: torch.Tensor, lengths: Tuple[int, int], pad_value: int = 0):\n",
    "    if tensor.size(0) >= lengths[0]:\n",
    "        tensor = tensor[:lengths[0], :]\n",
    "    if tensor.size(1) >= lengths[1]:\n",
    "        tensor = tensor[:, :lengths[1]]\n",
    "\n",
    "    if tensor.size(0) < lengths[0]:\n",
    "        tensor = torch.cat([tensor, torch.full((lengths[0] - tensor.size(0), tensor.size(1)), pad_value, dtype=tensor.dtype)], dim=0)\n",
    "    if tensor.size(1) < lengths[1]:\n",
    "        tensor = torch.cat([tensor, torch.full((tensor.size(0), lengths[1] - tensor.size(1)), pad_value, dtype=tensor.dtype)], dim=1)\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "def collate_fn(batch):\n",
    "    max_word_length = max(sentence[\"pos_tag_ids\"].size(0) for sentence in batch)\n",
    "    max_subword_length = max(sentence[\"subword_ids\"].size(0) for sentence in batch)\n",
    "\n",
    "    return {\n",
    "        \"subword_ids\": torch.stack(\n",
    "            [pad_1d(sentence[\"subword_ids\"], max_subword_length, 0) for sentence in batch],\n",
    "            dim=0\n",
    "        ),\n",
    "        \"pos_tag_ids\": torch.stack(\n",
    "            [pad_1d(sentence[\"pos_tag_ids\"], max_word_length, -1) for sentence in batch],\n",
    "            dim=0\n",
    "        ),\n",
    "        \"dependencies\": torch.stack(\n",
    "            [pad_1d(sentence[\"dependencies\"], max_word_length, -1) for sentence in batch],\n",
    "            dim=0\n",
    "        ),\n",
    "        \"dep_relation_ids\": torch.stack(\n",
    "            [pad_1d(sentence[\"dep_relation_ids\"], max_word_length, -1) for sentence in batch],\n",
    "            dim=0\n",
    "        ),\n",
    "        \"subword_to_word_map\": torch.stack(\n",
    "            [pad_1d(sentence[\"subword_to_word_map\"], max_subword_length, -1) for sentence in batch],\n",
    "            dim=0\n",
    "        ),\n",
    "        \"word_to_subword_map\": torch.stack(\n",
    "            [pad_1d(sentence[\"word_to_subword_map\"], max_word_length, -1) for sentence in batch],\n",
    "            dim=0\n",
    "        ),\n",
    "        \"attention_mask\": torch.stack(\n",
    "            [pad_1d(torch.ones(sentence[\"subword_ids\"].size(0), dtype=torch.bool), max_subword_length, False) for sentence in batch],\n",
    "            dim=0\n",
    "        )\n",
    "    }\n",
    "\n",
    "@dataclass\n",
    "class Sentence:\n",
    "    words: List[str] = field(default_factory=list) \n",
    "    subwords: List[str] = field(default_factory=list) \n",
    "    subword_ids: torch.LongTensor = None\n",
    "    subword_to_word_map: List[int] = field(default_factory=list)\n",
    "    word_to_subword_map: List[int] = field(default_factory=list)\n",
    "    pos_tags: List[str] = field(default_factory=list) \n",
    "    pos_tag_ids: torch.LongTensor = None\n",
    "    dependencies: List[int] = field(default_factory=list)\n",
    "    dep_relations: List[str] = field(default_factory=list)\n",
    "\n",
    "class ConlluDataset(Dataset):\n",
    "    def __init__(self, path: str, tokenizer: AutoTokenizer, pos_ids_to_str: List[str] = None, dep_ids_to_str: List[str] = None, verbose=True):\n",
    "        self.sentences = []\n",
    "        sentence = Sentence()\n",
    "        space_before = False\n",
    "        self.dep_relations = []\n",
    "\n",
    "        for line in open(path):\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            if len(line) == 0:\n",
    "                if len(sentence.words) > 0:\n",
    "                    self.sentences.append(sentence)\n",
    "                    sentence = Sentence()\n",
    "                continue\n",
    "            items = line.split(\"\\t\")\n",
    "            if not items[0].isdigit():\n",
    "                continue\n",
    "            word = (\"\" if space_before else \" \") + items[1].strip()\n",
    "            pos_tag = f\"POS={items[3].strip()}\" + (\"\" if items[5].strip() == \"_\" else f\"|{items[5].strip()}\")\n",
    "            dependency = int(items[6].strip())\n",
    "            dep_rel = items[7].strip()\n",
    "\n",
    "            sentence.words.append(word)\n",
    "            sentence.pos_tags.append(pos_tag)\n",
    "            sentence.dependencies.append(dependency)\n",
    "            sentence.dep_relations.append(dep_rel)\n",
    "            self.dep_relations.append(dep_rel)\n",
    "            space_before = \"SpaceAfter=No\" not in items[-1]\n",
    "        \n",
    "        if len(sentence.words) > 0:\n",
    "            self.sentences.append(sentence)\n",
    "\n",
    "        for sentence in self.sentences:\n",
    "            encoding = tokenizer(sentence.words, add_special_tokens=True, is_split_into_words=True)\n",
    "            sentence.subword_ids = torch.LongTensor(encoding.input_ids)\n",
    "            sentence.subwords = tokenizer.convert_ids_to_tokens(encoding.input_ids)\n",
    "            sentence.subword_to_word_map = encoding.word_ids()\n",
    "            sentence.word_to_subword_map = torch.LongTensor([\n",
    "                subword_index \n",
    "                for subword_index, word_index in enumerate(sentence.subword_to_word_map)\n",
    "                if word_index is not None and word_index != sentence.subword_to_word_map[subword_index - 1]\n",
    "            ])\n",
    "            sentence.subword_to_word_map = torch.LongTensor([\n",
    "                word_index if word_index is not None else -1 for word_index in sentence.subword_to_word_map\n",
    "            ])\n",
    "        \n",
    "        if pos_ids_to_str is None:\n",
    "            self.pos_ids_to_str = [\n",
    "                pos_tag \n",
    "                for pos_tag, count in Counter(tag for sentence in self.sentences for tag in sentence.pos_tags).most_common()\n",
    "            ]\n",
    "        else:\n",
    "            self.pos_ids_to_str = pos_ids_to_str\n",
    "\n",
    "        self.pos_str_to_ids = {tag: i for i, tag in enumerate(self.pos_ids_to_str)}\n",
    "        for sentence in self.sentences:\n",
    "            sentence.pos_tag_ids = torch.LongTensor([self.pos_str_to_ids.get(tag, 0) for tag in sentence.pos_tags])\n",
    "\n",
    "        if dep_ids_to_str is None:\n",
    "            self.dep_ids_to_str = [dep_rel for dep_rel, count in Counter(self.dep_relations).most_common()]\n",
    "        else:\n",
    "            self.dep_ids_to_str = dep_ids_to_str\n",
    "        self.dep_str_to_ids = {dep_rel: i for i, dep_rel in enumerate(self.dep_ids_to_str)}\n",
    "        for sentence in self.sentences:\n",
    "            sentence.dep_relation_ids = torch.LongTensor([self.dep_str_to_ids.get(dep_rel, 0) for dep_rel in sentence.dep_relations])\n",
    "\n",
    "    def state_dict(self):\n",
    "        return {\n",
    "            \"pos_vocabulary\": self.pos_ids_to_str,\n",
    "            \"dep_vocabulary\": self.dep_ids_to_str,\n",
    "        }\n",
    "\n",
    "    # load state dict\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.pos_ids_to_str = state_dict[\"pos_vocabulary\"]\n",
    "        self.pos_str_to_ids = {tag: i for i, tag in enumerate(self.pos_ids_to_str)}\n",
    "\n",
    "        for sentence in self.sentences:\n",
    "            sentence.pos_tag_ids = torch.LongTensor([self.pos_str_to_ids[tag] for tag in sentence.pos_tags])\n",
    "            sentence.dep_relation_ids = torch.LongTensor([self.dep_str_to_ids[dep_rel] for dep_rel in sentence.dep_relations])\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        sentence = self.sentences[index]\n",
    "        return {\n",
    "            \"words\": sentence.words,\n",
    "            \"subword_ids\": sentence.subword_ids,\n",
    "            \"pos_tag_ids\": sentence.pos_tag_ids,\n",
    "            \"dependencies\": torch.LongTensor(sentence.dependencies),\n",
    "            \"dep_relation_ids\": torch.LongTensor(sentence.dep_relation_ids),\n",
    "            \"subword_to_word_map\": sentence.subword_to_word_map,\n",
    "            \"word_to_subword_map\": sentence.word_to_subword_map\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8cda08-4767-4794-8334-e0cb6bf45884",
   "metadata": {},
   "source": [
    "# Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bebf36a6-65cd-4bec-a250-d531e2a4cfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_subword_to_word(subword_outputs, word_to_subword_map):\n",
    "    batch_size, word_seq_len = word_to_subword_map.shape\n",
    "    batch_indices = torch.arange(batch_size, device=subword_outputs.device).view(batch_size, 1).expand(batch_size, word_seq_len)\n",
    "    \n",
    "    word_outputs = subword_outputs[batch_indices, word_to_subword_map, :]\n",
    "    return word_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9f06bd-7414-49ab-9535-a448149e184d",
   "metadata": {},
   "source": [
    "# Combined Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4723c0f-c163-484f-a534-0d80f5a5c1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointModel(nn.Module):\n",
    "    def __init__(self, pretrained_model_name, num_pos_labels, num_dep_labels, max_seq_length=150):\n",
    "        super(JointModel, self).__init__()\n",
    "        self.base_model = AutoModel.from_pretrained(pretrained_model_name, trust_remote_code=True)\n",
    "        self.hidden_size = self.base_model.config.hidden_size\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "        self.pos_classifier = nn.Linear(self.hidden_size, num_pos_labels)\n",
    "        \n",
    "        self.dep_relation_classifier = nn.Linear(self.hidden_size, num_dep_labels)\n",
    "        \n",
    "        self.dep_head_projection = nn.Linear(self.hidden_size, 128)\n",
    "        self.dep_dependent_projection = nn.Linear(self.hidden_size, 128)\n",
    "        \n",
    "        self.root_embedding = nn.Parameter(torch.randn(1, 1, self.hidden_size))\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, word_to_subword_map=None):\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]\n",
    "        \n",
    "        pos_logits = self.pos_classifier(sequence_output)\n",
    "        dep_relation_logits = self.dep_relation_classifier(sequence_output)\n",
    "\n",
    "        if word_to_subword_map is not None:\n",
    "            word_representations = pool_subword_to_word(sequence_output, word_to_subword_map)\n",
    "            pos_logits = pool_subword_to_word(pos_logits, word_to_subword_map)\n",
    "            dep_relation_logits = pool_subword_to_word(dep_relation_logits, word_to_subword_map)\n",
    "            \n",
    "            batch_size, seq_len, _ = word_representations.shape\n",
    "            root_emb_expanded = self.root_embedding.expand(batch_size, -1, -1)\n",
    "            word_representations_with_root = torch.cat([root_emb_expanded, word_representations], dim=1)\n",
    "            \n",
    "            head_representations = self.dep_head_projection(word_representations_with_root)  # [batch, seq_len+1, 128]\n",
    "            dependent_representations = self.dep_dependent_projection(word_representations)  # [batch, seq_len, 128]\n",
    "            dep_head_scores = torch.bmm(dependent_representations, head_representations.transpose(1, 2))\n",
    "            \n",
    "            return pos_logits, dep_relation_logits, dep_head_scores\n",
    "        else:\n",
    "            return pos_logits, dep_relation_logits, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d103d72-c579-41c7-b245-c1b691d16cea",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4defc17c-c0b2-4612-afdd-33d5dbbc2439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_joint_model(model, train_loader, val_loader, device, epochs=5):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "    scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=50, \n",
    "        num_training_steps=len(train_loader) * epochs\n",
    "    )\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_pos_loss = 0.0\n",
    "        train_dep_loss = 0.0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Training]\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            pos_logits, dep_relation_logits, dep_head_scores = model(\n",
    "                input_ids=batch[\"subword_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                word_to_subword_map=batch[\"word_to_subword_map\"]\n",
    "            )\n",
    "            \n",
    "            pos_mask = batch[\"pos_tag_ids\"] != -1\n",
    "            dep_mask = batch[\"dep_relation_ids\"] != -1\n",
    "            \n",
    "            pos_loss = F.cross_entropy(\n",
    "                pos_logits.view(-1, pos_logits.size(-1))[pos_mask.view(-1)],\n",
    "                batch[\"pos_tag_ids\"].view(-1)[pos_mask.view(-1)]\n",
    "            )\n",
    "            \n",
    "            dep_relation_loss = F.cross_entropy(\n",
    "                dep_relation_logits.view(-1, dep_relation_logits.size(-1))[dep_mask.view(-1)],\n",
    "                batch[\"dep_relation_ids\"].view(-1)[dep_mask.view(-1)]\n",
    "            )\n",
    "            \n",
    "            dep_head_loss = F.cross_entropy(\n",
    "                dep_head_scores.view(-1, dep_head_scores.size(-1))[dep_mask.view(-1)],\n",
    "                batch[\"dependencies\"].view(-1)[dep_mask.view(-1)]\n",
    "            )\n",
    "            \n",
    "            loss = pos_loss + dep_relation_loss + dep_head_loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_pos_loss += pos_loss.item()\n",
    "            train_dep_loss += (dep_relation_loss + dep_head_loss).item()\n",
    "\n",
    "            progress_bar.set_postfix(pos_loss=pos_loss.item(), dep_loss=(dep_relation_loss + dep_head_loss).item())\n",
    "        \n",
    "        avg_train_pos_loss = train_pos_loss / len(train_loader)\n",
    "        avg_train_dep_loss = train_dep_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_pos_loss = 0.0\n",
    "        val_dep_loss = 0.0\n",
    "        pos_correct = 0\n",
    "        dep_rel_correct = 0\n",
    "        dep_head_correct = 0\n",
    "        total_pos = 0\n",
    "        total_dep = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Validation]\"):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                \n",
    "                pos_logits, dep_relation_logits, dep_head_scores = model(\n",
    "                    input_ids=batch[\"subword_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"],\n",
    "                    word_to_subword_map=batch[\"word_to_subword_map\"]\n",
    "                )\n",
    "                \n",
    "                pos_mask = batch[\"pos_tag_ids\"] != -1\n",
    "                dep_mask = batch[\"dep_relation_ids\"] != -1\n",
    "                \n",
    "                pos_loss = F.cross_entropy(\n",
    "                    pos_logits.view(-1, pos_logits.size(-1))[pos_mask.view(-1)],\n",
    "                    batch[\"pos_tag_ids\"].view(-1)[pos_mask.view(-1)]\n",
    "                )\n",
    "\n",
    "                dep_relation_loss = F.cross_entropy(\n",
    "                    dep_relation_logits.view(-1, dep_relation_logits.size(-1))[dep_mask.view(-1)],\n",
    "                    batch[\"dep_relation_ids\"].view(-1)[dep_mask.view(-1)]\n",
    "                )\n",
    "\n",
    "                dep_head_loss = F.cross_entropy(\n",
    "                    dep_head_scores.view(-1, dep_head_scores.size(-1))[dep_mask.view(-1)], batch[\"dependencies\"].view(-1)[dep_mask.view(-1)]\n",
    "                )\n",
    "\n",
    "                val_pos_loss += pos_loss.item()\n",
    "                val_dep_loss += (dep_relation_loss + dep_head_loss).item()\n",
    "\n",
    "                _, pos_predictions = pos_logits.max(dim=-1)\n",
    "                _, dep_relation_predictions = dep_relation_logits.max(dim=-1)\n",
    "                _, dep_head_predictions = dep_head_scores.max(dim=-1)\n",
    "                \n",
    "                pos_correct += (pos_predictions[pos_mask] == batch[\"pos_tag_ids\"][pos_mask]).sum().item()\n",
    "                total_pos += pos_mask.sum().item()\n",
    "                \n",
    "                dep_rel_correct += (dep_relation_predictions[dep_mask] == batch[\"dep_relation_ids\"][dep_mask]).sum().item()\n",
    "                \n",
    "                dep_head_correct += (dep_head_predictions[dep_mask] == batch[\"dependencies\"][dep_mask]).sum().item()\n",
    "                \n",
    "                total_dep += dep_mask.sum().item()\n",
    "        \n",
    "        avg_val_pos_loss = val_pos_loss / len(val_loader)\n",
    "        avg_val_dep_loss = val_dep_loss / len(val_loader)\n",
    "        pos_accuracy = pos_correct / total_pos if total_pos > 0 else 0\n",
    "        las = dep_rel_correct / total_dep if total_dep > 0 else 0\n",
    "        uas = dep_head_correct / total_dep if total_dep > 0 else 0\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Avg Train POS Loss = {avg_train_pos_loss:.4f} | Avg Train DEP Loss = {avg_train_dep_loss:.4f} | Avg Val POS Loss = {avg_val_pos_loss:.4f} | Avg Val DEP Loss = {avg_val_dep_loss:.4f}\")\n",
    "        #print(f\"Epoch {epoch+1}: POS Accuracy = {pos_accuracy:.4f} | LAS = {las:.4f} | UAS = {uas:.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6180e7-5f0d-4d69-9736-d982d1541356",
   "metadata": {},
   "source": [
    "# Predict output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81bbbde5-5e17-4eee-83ad-2bb851b66e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input_path, output_path, tokenizer, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model.eval()\n",
    "    dataset = ConlluDataset(\n",
    "        input_path, \n",
    "        tokenizer, \n",
    "        pos_ids_to_str=state_dict['pos_vocabulary'], \n",
    "        dep_ids_to_str=state_dict['dep_vocabulary']\n",
    "    )\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad(), open(output_path, \"w\") as output_file:\n",
    "        for batch in tqdm(dataloader, desc=\"Predicting\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            pos_logits, dep_relation_logits, dep_head_scores = model(\n",
    "                input_ids=batch[\"subword_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                word_to_subword_map=batch[\"word_to_subword_map\"]\n",
    "            )\n",
    "            \n",
    "            # Get the most likely class for each position\n",
    "            _, pos_predictions = pos_logits.max(dim=-1)\n",
    "            _, dep_relation_predictions = dep_relation_logits.max(dim=-1)\n",
    "            _, dep_head_predictions = dep_head_scores.max(dim=-1)\n",
    "            \n",
    "            for i in range(batch[\"pos_tag_ids\"].size(0)):\n",
    "                mask = batch[\"pos_tag_ids\"][i] != -1\n",
    "                sent_length = mask.sum().item()\n",
    "                \n",
    "                pos_tags = []\n",
    "                for j in range(sent_length):\n",
    "                    pred_idx = pos_predictions[i, j].item()\n",
    "                    if 0 <= pred_idx < len(dataset.pos_ids_to_str):\n",
    "                        pos_tags.append(dataset.pos_ids_to_str[pred_idx])\n",
    "                    else:\n",
    "                        pos_tags.append(dataset.pos_ids_to_str[0])\n",
    "                \n",
    "                dependencies = []\n",
    "                for j in range(sent_length):\n",
    "                    head_idx = dep_head_predictions[i, j].item()\n",
    "                    if head_idx > sent_length:\n",
    "                        head_idx = 0\n",
    "                    dependent_idx = j + 1\n",
    "                    \n",
    "                    dependencies.append([head_idx, dependent_idx])\n",
    "                \n",
    "                output_file.write(json.dumps({\n",
    "                    \"pos_tags\": pos_tags,\n",
    "                    \"dependencies\": dependencies\n",
    "                }) + \"\\n\")\n",
    "    \n",
    "    print(f\"Predictions saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1525898f",
   "metadata": {},
   "source": [
    "# Predict on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87b6733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_test(model, test_file_path, output_path, tokenizer, train_dataset, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Read test sentences\n",
    "    test_sentences = []\n",
    "    current_sentence = []\n",
    "    \n",
    "    with open(test_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            elif not line:\n",
    "                if current_sentence:\n",
    "                    test_sentences.append(current_sentence)\n",
    "                    current_sentence = []\n",
    "            else:\n",
    "                parts = line.split('\\t')\n",
    "                if parts[0].isdigit():\n",
    "                    # Only store word and SpaceAfter info\n",
    "                    word = parts[1].strip()\n",
    "                    space_after = \"SpaceAfter=No\" not in line\n",
    "                    current_sentence.append((word, space_after))\n",
    "    \n",
    "    # Add the last sentence if needed\n",
    "    if current_sentence:\n",
    "        test_sentences.append(current_sentence)\n",
    "    \n",
    "    with open(output_path, \"w\") as output_file:\n",
    "        for sentence in tqdm(test_sentences, desc=\"Predicting\"):\n",
    "            # Prepare input for the model\n",
    "            words = [word for word, _ in sentence]\n",
    "            encoding = tokenizer(words, add_special_tokens=True, is_split_into_words=True, return_tensors=\"pt\")\n",
    "            \n",
    "            input_ids = encoding.input_ids.to(device)\n",
    "            attention_mask = encoding.attention_mask.to(device)\n",
    "            \n",
    "            # Create word to subword map\n",
    "            word_to_subword_map = []\n",
    "            for word_idx, word_id in enumerate(encoding.word_ids(batch_index=0)):\n",
    "                if word_id is not None and (word_idx == 0 or encoding.word_ids(batch_index=0)[word_idx-1] != word_id):\n",
    "                    word_to_subword_map.append(word_idx)\n",
    "            \n",
    "            word_to_subword_map = torch.tensor([word_to_subword_map]).to(device)\n",
    "            \n",
    "            # Make prediction\n",
    "            with torch.no_grad():\n",
    "                pos_logits, dep_relation_logits, dep_head_scores = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    word_to_subword_map=word_to_subword_map\n",
    "                )\n",
    "                \n",
    "                _, pos_predictions = pos_logits.max(dim=-1)\n",
    "                _, dep_relation_predictions = dep_relation_logits.max(dim=-1)\n",
    "                _, dep_head_predictions = dep_head_scores.max(dim=-1)\n",
    "            \n",
    "            # Convert predictions to output format\n",
    "            sent_length = len(sentence)\n",
    "            pos_tags = [train_dataset.pos_ids_to_str[pos_predictions[0, j].item()] \n",
    "                        for j in range(sent_length)]\n",
    "            \n",
    "            dependencies = []\n",
    "            for j in range(sent_length):\n",
    "                # Get head prediction (1-indexed)\n",
    "                head_idx = dep_head_predictions[0, j].item()\n",
    "                if head_idx >= sent_length:\n",
    "                    head_idx = 0  # Set to root if out of bounds\n",
    "                dependent_idx = j + 1  # 1-indexed\n",
    "                \n",
    "                # Format exactly as [head_idx, dependent_idx]\n",
    "                dependencies.append([head_idx, dependent_idx])\n",
    "            \n",
    "            # Output in the exact required format\n",
    "            result = {\n",
    "                \"pos_tags\": pos_tags,\n",
    "                \"dependencies\": dependencies\n",
    "            }\n",
    "            \n",
    "            output_file.write(json.dumps(result) + \"\\n\")\n",
    "    \n",
    "    print(f\"Predictions saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410ad5e7-3d8e-488d-b5f9-b4bf810d4691",
   "metadata": {},
   "source": [
    "# Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eacaf64c-ccec-4f20-af07-e65da44fe8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Training]: 100%|██████████| 490/490 [00:42<00:00, 11.51it/s, dep_loss=1.71, pos_loss=0.2]   \n",
      "Epoch 1/10 [Validation]: 100%|██████████| 76/76 [00:01<00:00, 40.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Avg Train POS Loss = 1.0993 | Avg Train DEP Loss = 12.6982 | Avg Val POS Loss = 4.0859 | Avg Val DEP Loss = 2.7880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Training]: 100%|██████████| 490/490 [00:42<00:00, 11.63it/s, dep_loss=0.404, pos_loss=0.151] \n",
      "Epoch 2/10 [Validation]: 100%|██████████| 76/76 [00:01<00:00, 40.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Avg Train POS Loss = 0.1528 | Avg Train DEP Loss = 0.8445 | Avg Val POS Loss = 4.5922 | Avg Val DEP Loss = 2.9640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Training]: 100%|██████████| 490/490 [00:42<00:00, 11.53it/s, dep_loss=0.407, pos_loss=0.0621]\n",
      "Epoch 3/10 [Validation]: 100%|██████████| 76/76 [00:01<00:00, 40.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Avg Train POS Loss = 0.0871 | Avg Train DEP Loss = 0.4933 | Avg Val POS Loss = 5.0928 | Avg Val DEP Loss = 3.2441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Training]: 100%|██████████| 490/490 [00:42<00:00, 11.53it/s, dep_loss=0.423, pos_loss=0.0318]\n",
      "Epoch 4/10 [Validation]: 100%|██████████| 76/76 [00:01<00:00, 40.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Avg Train POS Loss = 0.0553 | Avg Train DEP Loss = 0.3188 | Avg Val POS Loss = 5.4173 | Avg Val DEP Loss = 3.3963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Training]: 100%|██████████| 490/490 [00:42<00:00, 11.52it/s, dep_loss=0.112, pos_loss=0.0154] \n",
      "Epoch 5/10 [Validation]: 100%|██████████| 76/76 [00:01<00:00, 40.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Avg Train POS Loss = 0.0357 | Avg Train DEP Loss = 0.2070 | Avg Val POS Loss = 5.9961 | Avg Val DEP Loss = 3.8768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Training]: 100%|██████████| 490/490 [00:42<00:00, 11.49it/s, dep_loss=0.148, pos_loss=0.0292]  \n",
      "Epoch 6/10 [Validation]: 100%|██████████| 76/76 [00:01<00:00, 40.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Avg Train POS Loss = 0.0236 | Avg Train DEP Loss = 0.1433 | Avg Val POS Loss = 6.1778 | Avg Val DEP Loss = 3.9649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Training]: 100%|██████████| 490/490 [00:42<00:00, 11.48it/s, dep_loss=0.0764, pos_loss=0.0128] \n",
      "Epoch 7/10 [Validation]: 100%|██████████| 76/76 [00:01<00:00, 40.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Avg Train POS Loss = 0.0152 | Avg Train DEP Loss = 0.0908 | Avg Val POS Loss = 6.5748 | Avg Val DEP Loss = 4.2632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Training]: 100%|██████████| 490/490 [00:42<00:00, 11.50it/s, dep_loss=0.0818, pos_loss=0.015]   \n",
      "Epoch 8/10 [Validation]: 100%|██████████| 76/76 [00:01<00:00, 40.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Avg Train POS Loss = 0.0097 | Avg Train DEP Loss = 0.0545 | Avg Val POS Loss = 7.0951 | Avg Val DEP Loss = 4.7886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Training]: 100%|██████████| 490/490 [00:42<00:00, 11.49it/s, dep_loss=0.0442, pos_loss=0.00289] \n",
      "Epoch 9/10 [Validation]: 100%|██████████| 76/76 [00:01<00:00, 40.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Avg Train POS Loss = 0.0060 | Avg Train DEP Loss = 0.0288 | Avg Val POS Loss = 7.7022 | Avg Val DEP Loss = 5.2440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Training]: 100%|██████████| 490/490 [00:42<00:00, 11.56it/s, dep_loss=0.004, pos_loss=0.00268]   \n",
      "Epoch 10/10 [Validation]: 100%|██████████| 76/76 [00:01<00:00, 40.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Avg Train POS Loss = 0.0034 | Avg Train DEP Loss = 0.0141 | Avg Val POS Loss = 7.9397 | Avg Val DEP Loss = 5.4044\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "predict_on_test() missing 1 required positional argument: 'state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m     joint_model \u001b[38;5;241m=\u001b[39m train_joint_model(joint_model, train_loader, val_loader, device, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Predict using the joint model\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     \u001b[43mpredict_on_test\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/fp/projects01/ec403/IN5550/obligatories/2/test.conllu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredictions-test.jsonl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoint_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Models:\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# ltg/norbert3-xs\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# xlm-roberta-base\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# bert-base-german-cased\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Path:\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# /fp/projects01/ec403/IN5550/obligatories/2/no_bokmaal-ud-dev.jsonl and /fp/projects01/ec403/IN5550/obligatories/2/no_bokmaal-ud-dev.conllu\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: predict_on_test() missing 1 required positional argument: 'state_dict'"
     ]
    }
   ],
   "source": [
    "gradient_clipping = 5\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"ltg/norbert3-large\")\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load datasets\n",
    "    train_data = ConlluDataset('/fp/projects01/ec403/IN5550/obligatories/2/no_bokmaal-ud-train.conllu', tokenizer)\n",
    "    state_dict = train_data.state_dict()\n",
    "    val_data = ConlluDataset('/fp/projects01/ec403/IN5550/obligatories/2/no_bokmaal-ud-dev.conllu', tokenizer)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_data,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_data,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    #Joint model \n",
    "    joint_model = JointModel(\n",
    "        pretrained_model_name=\"ltg/norbert3-large\",\n",
    "        num_pos_labels=len(train_data.pos_ids_to_str),\n",
    "        num_dep_labels=len(train_data.dep_ids_to_str)\n",
    "    ).to(device)\n",
    "    \n",
    "    # Train joint model\n",
    "    joint_model = train_joint_model(joint_model, train_loader, val_loader, device, epochs=10)\n",
    "    \n",
    "    # Predict using the joint model\n",
    "    predict(\n",
    "        model=joint_model,\n",
    "        input_path='/fp/projects01/ec403/IN5550/obligatories/2/no_bokmaal-ud-dev.conllu',\n",
    "        output_path='predictions.jsonl',\n",
    "        tokenizer=tokenizer,\n",
    "        device=device\n",
    "    )\n",
    "# Models:\n",
    "# ltg/norbert3-xs\n",
    "# xlm-roberta-base\n",
    "# bert-base-german-cased\n",
    "# Path:\n",
    "# /fp/projects01/ec403/IN5550/obligatories/2/no_bokmaal-ud-dev.jsonl and /fp/projects01/ec403/IN5550/obligatories/2/no_bokmaal-ud-dev.conllu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5eb4e12a-b441-4fac-b5e8-c15495e657e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 1939/1939 [00:21<00:00, 89.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to test_predictions.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predict_on_test(\n",
    "        model=joint_model,\n",
    "        test_file_path='/fp/projects01/ec403/IN5550/obligatories/2/test.conllu',\n",
    "        output_path='test_predictions.jsonl',\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=train_data,  \n",
    "        device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55378ee6-6d03-4a0a-a1a9-ff4fd1f997dd",
   "metadata": {},
   "source": [
    "# Evaluer output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7377848d-6366-4461-86b8-35911d7ef814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_accuracy(gold: List[str], prediction: List[str]) -> float:\n",
    "    if len(gold) != len(prediction):\n",
    "        return 0.0\n",
    "    return mean(1 if g == p else 0 for g, p in zip(gold, prediction))\n",
    "\n",
    "\n",
    "def unlabeled_attachment_score(gold: List[Tuple[int, int]], prediction: List[Tuple[int, int]]) -> float:\n",
    "    gold = set(tuple(dependency) for dependency in gold)\n",
    "    prediction = set(tuple(dependency) for dependency in prediction)\n",
    "\n",
    "    precision = len(gold & prediction) / len(prediction)\n",
    "    recall = len(gold & prediction) / len(gold)\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0.0\n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "def labeled_attachment_score(gold: List[Tuple[int, int, str, str]], prediction: List[Tuple[int, int, str, str]]) -> float:\n",
    "    gold = set(gold)\n",
    "    prediction = set(prediction)\n",
    "\n",
    "    precision = len(gold & prediction) / len(prediction)\n",
    "    recall = len(gold & prediction) / len(gold)\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0.0\n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "def create_labeled_dependencies(pos_tags: List[str], dependencies: List[Tuple[int, int]]) -> List[Tuple[int, int, str, str]]:\n",
    "    assert min(min(head, dependency) for head, dependency in dependencies) >= 0, \"Dependency index out of bounds\"\n",
    "    assert max(max(head, dependency) for head, dependency in dependencies) <= len(pos_tags), \"Dependency index out of bounds\"\n",
    "    \n",
    "    pos_tags = [\"ROOT\"] + pos_tags\n",
    "    \n",
    "    return [(head, dependency, pos_tags[head], pos_tags[dependency]) for head, dependency in dependencies]\n",
    "\n",
    "\n",
    "def sentence_metrics(sentence_gold, sentence_prediction):\n",
    "    pos_acc = pos_accuracy(sentence_gold[\"pos_tags\"], sentence_prediction[\"pos_tags\"])\n",
    "    uas = unlabeled_attachment_score(sentence_gold[\"dependencies\"], sentence_prediction[\"dependencies\"])\n",
    "    las = labeled_attachment_score(\n",
    "        create_labeled_dependencies(sentence_gold[\"pos_tags\"], sentence_gold[\"dependencies\"]),\n",
    "        create_labeled_dependencies(sentence_prediction[\"pos_tags\"], sentence_prediction[\"dependencies\"])\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"POS accuracy\": pos_acc,\n",
    "        \"Unlabeled attachment score\": uas,\n",
    "        \"Labeled attachment score\": las\n",
    "    }\n",
    "\n",
    "\n",
    "def dataset_metrics(gold_path: str, prediction_path: str, verbose=True):\n",
    "    gold_sentences = [json.loads(line) for line in open(gold_path)]\n",
    "    prediction_sentences = [json.loads(line) for line in open(prediction_path)]\n",
    "\n",
    "    assert len(gold_sentences) == len(prediction_sentences), \"Number of sentences do not match\"\n",
    "\n",
    "    metrics = [sentence_metrics(gold, prediction) for gold, prediction in zip(gold_sentences, prediction_sentences)]\n",
    "\n",
    "    metrics = {\n",
    "        \"POS accuracy\": mean(metric[\"POS accuracy\"] for metric in metrics),\n",
    "        \"Unlabeled attachment score\": mean(metric[\"Unlabeled attachment score\"] for metric in metrics),\n",
    "        \"Labeled attachment score\": mean(metric[\"Labeled attachment score\"] for metric in metrics)\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        print(\"METRICS\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value:.2%}\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3de85140-f2ec-4c97-9c24-de3e75f3ffc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRICS\n",
      "POS accuracy: 97.02%\n",
      "Unlabeled attachment score: 95.66%\n",
      "Labeled attachment score: 90.01%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'POS accuracy': 0.9701674328484712,\n",
       " 'Unlabeled attachment score': 0.9566305955258916,\n",
       " 'Labeled attachment score': 0.9001210377290003}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_metrics(\"/fp/projects01/ec403/IN5550/obligatories/2/no_bokmaal-ud-dev.jsonl\", \"predictions.jsonl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aea9e2-3766-445a-a09f-dd6aa49db1a7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
